
FastAPI-based Face Verification Service with Evaluation on LFW, CALFW, and CPLFW datasets

# ğŸ§  Overview

This project implements a complete face verification pipeline composed of:

1. REST API Server (FastAPI)
2. Client-side evaluation framework
3. Benchmark testing on standard datasets

The system evaluates deep face recognition models using cosine similarity and reports:

- Accuracy
- False Match Rate (FMR)
- False Non-Match Rate (FNMR)
- Runtime performance


# ğŸ— Project Structure

```text
face-verification-project/
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ model.py
â”‚       â”œâ”€â”€ schemas.py
â”‚       â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ datasets/                 
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ lfw_loader.py
â”‚   â”œâ”€â”€ calfw_loader.py
â”‚   â”œâ”€â”€ cplfw_loader.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ results.json
â”‚
â””â”€â”€ README.md
```


## Server â€“ Face Verification API

Endpoint:

POST /verify

### Request Parameters

- image1 â†’ first image file
- image2 â†’ second image file
- model_name â†’ string (e.g., "buffalo_l")

### Response Example

```

{
"similarity": 0.8734
}

```


## Client â€“ Evaluation Framework

The client:

- Loads dataset pairs
- Sends image pairs to the server
- Collects similarity scores
- Computes evaluation metrics
- Measures runtime

# ğŸ“š Supported Datasets

- LFW
- CALFW (Cross-Age)
- CPLFW (Cross-Pose)

Each dataset contains 6000 verification pairs.

# ğŸ“Š Evaluation Metrics

Accuracy  
FMR (False Match Rate)  
FNMR (False Non-Match Rate)

# âš™ï¸ Installation

## Clone Repository

```

git clone https://github.com/mehranghaffarian/insightface-server-evaluation

```

## Setup Server

```

cd server
python -m venv venv
source venv/bin/activate      # Linux/macOS
venv\Scripts\activate         # Windows

pip install -r requirements.txt

```

## Run Server

```

uvicorn app.main:app --reload

```

Server runs at:

http://127.0.0.1:8000

## Run Evaluation

Open new terminal:

```

cd client
python evaluate.py

```


# ğŸ§ª Example Results (threshold = 0.4)

| Dataset | Model | Accuracy | FMR | FNMR |
|----------|--------|----------|------|--------|
| LFW | buffalo_l | 0.979 | 0.000 | 0.042 |
| CALFW | buffalo_l | 0.5427 | 0.000 | 0.1467 |
| CPLFW | buffalo_l | 0.8478 | 0.000 | 0.3043 |
| LFW | buffalo_s | 0.979 | 0.000 | 0.042 |
| CALFW | buffalo_s | 0.5427 | 0.000 | 0.1467 |
| CPLFW | buffalo_s | 0.8478 | 0.000 | 0.3043 |


# ğŸ“ˆ Observations

- High performance on LFW.
- Significant performance drop on CALFW (age variation).
- Moderate drop on CPLFW (pose variation).
- FMR = 0 due to conservative threshold (0.4).
- Average inference time per pair: ~0.6â€“0.8 seconds.

# ğŸ›  Technologies

- Python
- FastAPI
- Uvicorn
- InsightFace
- NumPy
- OpenCV
- Requests
- tqdm

# ğŸ“Œ Notes

- venv is excluded from version control.
- Datasets are not included due to size.
- Ensure datasets are extracted and properly placed before evaluation.

